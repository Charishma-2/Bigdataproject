{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e31dadb-43cb-4903-af4f-f719afc5aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|           Country|        TotalSales|\n",
      "+------------------+------------------+\n",
      "|     United States| 2272449.856299968|\n",
      "|         Australia| 925235.8529999997|\n",
      "|            France| 858931.0830000006|\n",
      "|             China| 700562.0250000003|\n",
      "|           Germany| 628840.0305000001|\n",
      "|            Mexico| 622590.6175199997|\n",
      "|             India| 589650.1049999999|\n",
      "|    United Kingdom| 528576.2999999995|\n",
      "|         Indonesia|       404887.4979|\n",
      "|            Brazil| 361106.4189600004|\n",
      "|             Italy|        289709.658|\n",
      "|             Spain|287146.68000000005|\n",
      "|       Philippines|        183420.165|\n",
      "|       El Salvador|177554.90319999988|\n",
      "|       New Zealand|172020.62400000013|\n",
      "|              Cuba|158854.93548000007|\n",
      "|         Nicaragua|149687.06456000003|\n",
      "|         Guatemala|131602.46675999995|\n",
      "|Dominican Republic|126140.58128000004|\n",
      "|              Iran|113746.10999999999|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT Country, SUM(Sales) AS TotalSales FROM sales_data  GROUP BY Country ORDER BY TotalSales DESC\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdf3784-8628-440e-be22-c0a94b7f2e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|        Region|     AverageProfit|\n",
      "+--------------+------------------+\n",
      "|    North Asia| 70.82053934987178|\n",
      "|  Central Asia| 64.68759130859378|\n",
      "|        Canada|46.399453125000015|\n",
      "|         North| 40.66832863531873|\n",
      "|       Oceania|34.439091482649815|\n",
      "|          West| 33.50099953168908|\n",
      "|          East| 32.16399462780898|\n",
      "|       Central|28.051463716830032|\n",
      "|         South|21.107099605718602|\n",
      "|     Caribbean|20.456402982248513|\n",
      "|        Africa|19.374674296926074|\n",
      "|          EMEA| 8.728966196062856|\n",
      "|Southeast Asia| 5.705442313838281|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT Region, AVG(Profit) AS AverageProfit FROM sales_data GROUP BY Region ORDER BY AverageProfit DESC\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b341480f-346c-4df4-bf58-add51b702e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    Segment| TotalShippingCost|\n",
      "+-----------+------------------+\n",
      "|   Consumer| 696737.5118000008|\n",
      "|  Corporate| 410810.3044999995|\n",
      "|Home Office|245140.97340000072|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT Segment, SUM(ShippingCost) AS TotalShippingCost FROM sales_data GROUP BY Segment ORDER BY TotalShippingCost DESC\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3971cf60-df1d-4ed8-8db1-786d8b5e687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|Market|NumberOfCustomers|\n",
      "+------+-----------------+\n",
      "|  APAC|            11002|\n",
      "| LATAM|            10294|\n",
      "|    EU|            10000|\n",
      "|    US|             9994|\n",
      "|  EMEA|             5029|\n",
      "|Africa|             4587|\n",
      "|Canada|              384|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT Market, COUNT(CustomerID) AS NumberOfCustomers FROM sales_data GROUP BY Market ORDER BY NumberOfCustomers DESC\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0279866-1e38-488a-8a57-1e2bb2e81f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|SubCategory|NumberOfOrders|\n",
      "+-----------+--------------+\n",
      "|    Binders|          6152|\n",
      "|    Storage|          5059|\n",
      "|        Art|          4883|\n",
      "|      Paper|          3538|\n",
      "|     Chairs|          3434|\n",
      "|     Phones|          3357|\n",
      "|Furnishings|          3170|\n",
      "|Accessories|          3075|\n",
      "|     Labels|          2606|\n",
      "|  Envelopes|          2435|\n",
      "|   Supplies|          2425|\n",
      "|  Fasteners|          2420|\n",
      "|  Bookcases|          2411|\n",
      "|    Copiers|          2223|\n",
      "| Appliances|          1755|\n",
      "|   Machines|          1486|\n",
      "|     Tables|           861|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT SubCategory, COUNT(OrderID) AS NumberOfOrders FROM sales_data GROUP BY SubCategory ORDER BY NumberOfOrders DESC\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "588820d0-f9ee-46e3-b149-d1ebe2f9e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|OrderPriority|TotalQuantity|\n",
      "+-------------+-------------+\n",
      "|       Medium|     101846.0|\n",
      "|         High|      53727.0|\n",
      "|     Critical|      13363.0|\n",
      "|          Low|       8252.0|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT OrderPriority, SUM(Quantity) AS TotalQuantity FROM sales_data GROUP BY OrderPriority ORDER BY TotalQuantity DESC limit 4\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6475220e-55f7-4987-84a4-e65f382b903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+------------------+\n",
      "|      ShipMode|      Region| TotalShippingCost|\n",
      "+--------------+------------+------------------+\n",
      "|   First Class|      Africa| 19796.19999999999|\n",
      "|      Same Day|      Africa| 8798.429999999998|\n",
      "|  Second Class|      Africa|          23024.63|\n",
      "|Standard Class|      Africa| 36520.21000000001|\n",
      "|   First Class|      Canada|1934.4399999999998|\n",
      "|      Same Day|      Canada|            631.53|\n",
      "|  Second Class|      Canada|2688.2900000000004|\n",
      "|Standard Class|      Canada|2151.3700000000003|\n",
      "|   First Class|   Caribbean| 7474.250000000001|\n",
      "|      Same Day|   Caribbean|3097.5699999999997|\n",
      "|  Second Class|   Caribbean|           9550.09|\n",
      "|Standard Class|   Caribbean|15468.239999999998|\n",
      "|   First Class|     Central| 67350.72159999987|\n",
      "|      Same Day|     Central|24589.182800000002|\n",
      "|  Second Class|     Central| 68078.09319999996|\n",
      "|Standard Class|     Central|136221.03329999992|\n",
      "|   First Class|Central Asia|14781.250000000005|\n",
      "|      Same Day|Central Asia|5400.9800000000005|\n",
      "|  Second Class|Central Asia|18349.789999999997|\n",
      "|Standard Class|Central Asia| 39298.13000000005|\n",
      "+--------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"SELECT ShipMode, Region, SUM(ShippingCost) AS TotalShippingCost FROM sales_data GROUP BY ShipMode, Region ORDER BY Region, ShipMode\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "076ff629-0d13-425f-8740-63b8234f8a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|NumberOfCustomers|\n",
      "+----+-----------------+\n",
      "|2011|             1309|\n",
      "|2012|             1373|\n",
      "|2013|             1458|\n",
      "|2014|             1511|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "salesdata=spark.read.format('csv').option('header','true').load('Sales.csv')\n",
    "salesdata.createOrReplaceTempView('sales_data')\n",
    "result_df = spark.sql(\"\"\"SELECT YEAR(TO_DATE(OrderDate, 'M/d/yyyy')) AS Year, COUNT(DISTINCT CustomerID) AS NumberOfCustomers FROM sales_data WHERE OrderDate \n",
    "IS NOT NULL GROUP BY YEAR(TO_DATE(OrderDate, 'M/d/yyyy')) ORDER BY Year\"\"\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18cc82bc-f56c-42bc-87b4-4c3824ba5c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+------+------------+----------------+---------------+-----------+--------------------+--------+--------+--------+----------+------------+-------------+\n",
      "|RowID|        OrderID| OrderDate|  ShipDate|      ShipMode|CustomerID|    CustomerName|    Segment|         City|          State|      Country|Market|      Region|       ProductID|       Category|SubCategory|         ProductName|   Sales|Quantity|Discount|    Profit|ShippingCost|OrderPriority|\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+------+------------+----------------+---------------+-----------+--------------------+--------+--------+--------+----------+------------+-------------+\n",
      "|32298| CA-2012-124891| 7/31/2012| 7/31/2012|      Same Day|  RH-19495|     Rick Hansen|   Consumer|New York City|       New York|United States|    US|        East| TEC-AC-10003033|     Technology|Accessories|Plantronics CS510...| 2309.65|       7|       0|  762.1845|      933.57|     Critical|\n",
      "|26341|  IN-2013-77878|  5/2/2013|  7/2/2013|  Second Class|  JR-16210|   Justin Ritter|  Corporate|   Wollongong|New South Wales|    Australia|  APAC|     Oceania| FUR-CH-10003950|      Furniture|     Chairs|Novimex Executive...|3709.395|       9|     0.1|  -288.765|      923.63|     Critical|\n",
      "|25330|  IN-2013-71249|10/17/2013|10/18/2013|   First Class|  CR-12730|    Craig Reiter|   Consumer|     Brisbane|     Queensland|    Australia|  APAC|     Oceania| TEC-PH-10004664|     Technology|     Phones|Nokia Smart Phone...|5175.171|       9|     0.1|   919.971|      915.49|       Medium|\n",
      "|13524|ES-2013-1579342| 1/28/2013| 1/30/2013|   First Class|  KM-16375|Katherine Murray|Home Office|       Berlin|         Berlin|      Germany|    EU|     Central| TEC-PH-10004583|     Technology|     Phones|Motorola Smart Ph...| 2892.51|       5|     0.1|    -96.54|      910.16|       Medium|\n",
      "|47221|   SG-2013-4320| 5/11/2013| 6/11/2013|      Same Day|   RH-9495|     Rick Hansen|   Consumer|        Dakar|          Dakar|      Senegal|Africa|      Africa|TEC-SHA-10000501|     Technology|    Copiers|Sharp Wireless Fa...| 2832.96|       8|       0|    311.52|      903.04|     Critical|\n",
      "|22732|  IN-2013-42360| 6/28/2013|  1/7/2013|  Second Class|  JM-15655|     Jim Mitchum|  Corporate|       Sydney|New South Wales|    Australia|  APAC|     Oceania| TEC-PH-10000030|     Technology|     Phones|Samsung Smart Pho...|2862.675|       5|     0.1|   763.275|      897.35|     Critical|\n",
      "|30570|  IN-2011-81826| 7/11/2011| 9/11/2011|   First Class|  TS-21340|   Toby Swindell|   Consumer|      Porirua|     Wellington|  New Zealand|  APAC|     Oceania| FUR-CH-10004050|      Furniture|     Chairs|Novimex Executive...| 1822.08|       4|       0|    564.84|      894.77|     Critical|\n",
      "|31192|  IN-2012-86369| 4/14/2012| 4/18/2012|Standard Class|  MB-18085|      Mick Brown|   Consumer|     Hamilton|        Waikato|  New Zealand|  APAC|     Oceania| FUR-TA-10002958|      Furniture|     Tables|Chromcraft Confer...| 5244.84|       6|       0|    996.48|      878.38|         High|\n",
      "|40155| CA-2014-135909|10/14/2014|10/21/2014|Standard Class|  JW-15220|       Jane Waco|  Corporate|   Sacramento|     California|United States|    US|        West| OFF-BI-10003527|Office Supplies|    Binders|Fellowes PB500 El...| 5083.96|       5|     0.2|  1906.485|      867.69|          Low|\n",
      "|40936| CA-2012-116638| 1/28/2012| 1/31/2012|  Second Class|  JH-15985|     Joseph Holt|   Consumer|      Concord| North Carolina|United States|    US|       South| FUR-TA-10000198|      Furniture|     Tables|Chromcraft Bull-N...|4297.644|      13|     0.4|-1862.3124|      865.74|     Critical|\n",
      "|34577| CA-2011-102988|  5/4/2011|  9/4/2011|  Second Class|  GM-14695|    Greg Maxwell|  Corporate|   Alexandria|       Virginia|United States|    US|       South| OFF-SU-10002881|Office Supplies|   Supplies|Martin Yale Chadl...| 4164.05|       5|       0|    83.281|      846.54|         High|\n",
      "|28879|  ID-2012-28402| 4/19/2012| 4/22/2012|   First Class|  AJ-10780|  Anthony Jacobs|  Corporate|        Kabul|          Kabul|  Afghanistan|  APAC|Central Asia| FUR-TA-10001889|      Furniture|     Tables|Bevis Conference ...| 4626.15|       5|       0|    647.55|      835.57|         High|\n",
      "|45794|   SA-2011-1830|12/27/2011|12/29/2011|  Second Class|   MM-7260| Magdelene Morse|   Consumer|        Jizan|          Jizan| Saudi Arabia|  EMEA|        EMEA|TEC-CIS-10001717|     Technology|     Phones|Cisco Smart Phone...| 2616.96|       4|       0|    1151.4|      832.41|     Critical|\n",
      "| 4132| MX-2012-130015|11/13/2012|11/13/2012|      Same Day|  VF-21715|  Vicky Freymann|Home Office|       Toledo|         Parana|       Brazil| LATAM|       South| FUR-CH-10002033|      Furniture|     Chairs|Harbour Creations...|  2221.8|       7|       0|    622.02|      810.25|     Critical|\n",
      "|27704|  IN-2013-73951|  6/6/2013|  8/6/2013|  Second Class|  PF-19120|    Peter Fuller|   Consumer|   Mudanjiang|   Heilongjiang|        China|  APAC|  North Asia| OFF-AP-10003500|Office Supplies| Appliances|KitchenAid Microw...| 3701.52|      12|       0|   1036.08|      804.54|     Critical|\n",
      "|13779|ES-2014-5099955| 7/31/2014|  3/8/2014|  Second Class|  BP-11185|    Ben Peterman|  Corporate|        Paris|  Ile-de-France|       France|    EU|     Central| OFF-AP-10000423|Office Supplies| Appliances|Breville Refriger...|1869.588|       4|     0.1|   186.948|      801.66|     Critical|\n",
      "|36178| CA-2014-143567| 3/11/2014| 6/11/2014|  Second Class|  TB-21175|   Thomas Boland|  Corporate|    Henderson|       Kentucky|United States|    US|       South| TEC-AC-10004145|     Technology|Accessories|Logitech diNovo E...| 2249.91|       9|       0|  517.4793|       780.7|     Critical|\n",
      "|12069|ES-2014-1651774|  8/9/2014| 9/14/2014|Standard Class|  PJ-18835|   Patrick Jones|  Corporate|        Prato|        Tuscany|        Italy|    EU|       South| OFF-AP-10004512|Office Supplies| Appliances|   Hoover Stove, Red| 7958.58|      14|       0|   3979.08|      778.32|          Low|\n",
      "|22096|  IN-2014-11763| 1/31/2014|  1/2/2014|   First Class|  JS-15685|        Jim Sink|  Corporate|   Townsville|     Queensland|    Australia|  APAC|     Oceania| TEC-CO-10000865|     Technology|    Copiers|Brother Fax Machi...|2565.594|       9|     0.1|    28.404|      766.93|     Critical|\n",
      "|49463|   TZ-2014-8190| 5/12/2014| 7/12/2014|  Second Class|   RH-9555| Ritsa Hightower|   Consumer|       Uvinza|         Kigoma|     Tanzania|Africa|      Africa|OFF-KIT-10004058|Office Supplies| Appliances|KitchenAid Stove,...| 3409.74|       6|       0|    818.28|      763.38|         High|\n",
      "+-----+---------------+----------+----------+--------------+----------+----------------+-----------+-------------+---------------+-------------+------+------------+----------------+---------------+-----------+--------------------+--------+--------+--------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salesdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be738d5f-67f4-45c9-9560-bdb97dd13c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
